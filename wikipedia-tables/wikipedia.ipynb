{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.sparql import (get_subclasses_of_item, return_sparql_query_results)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful Soup Scrape for Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "\n",
    "from itertools import chain\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put wikidata results into a dataframe\n",
    "def niceify(results):\n",
    "    count = 0 \n",
    "    noice = []\n",
    "    for res in results['results']['bindings']:\n",
    "        temp={}\n",
    "        for h in headers:\n",
    "            if h in res.keys():\n",
    "                temp[h] = res[h].get('value', \"None\")\n",
    "        \n",
    "            noice.append(temp)\n",
    "    df = pd.DataFrame(noice)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any duplicate rows and delete rows without a URL\n",
    "def de_dupe(df, NaN_column='article'):\n",
    "    \n",
    "    df = df.dropna(subset=[NaN_column])\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of dicts; dict values are of dataframes of the tables for that particular url\n",
    "def table_scrapes(url_list, prob=.05, maxdelay=3):\n",
    "    \n",
    "    print(\"Starting your scrape...\")\n",
    "    \n",
    "    #Add key to tuple in \"amplified\" list of urls from wikidata search\n",
    "    urls_amp = [(x, x.split(\"/\")[-1]) for x in url_list]\n",
    "    \n",
    "    super_list_dict = []\n",
    "    count = 0\n",
    "    success = 0\n",
    "    tot = len(url_list)\n",
    "    delay = [decision(prob,maxdelay) for x in range(tot)]\n",
    "    \n",
    "    # Iterate over each url, pull out any wikitables and throw into super list with key=article url tag\n",
    "    for url_tup in urls_amp:\n",
    "\n",
    "        if delay[count] != 0:\n",
    "            print(f'Pause scrape for {delay[count]} second(s)')\n",
    "            time.sleep(delay[count])\n",
    "        \n",
    "        # unique keys for the url\n",
    "        url = url_tup[0]\n",
    "        key = url_tup[1]\n",
    "\n",
    "        #soup it \n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Filter table search to just wikitables which will clean out junk tables\n",
    "        #tables = soup.find_all(\"table\",{\"class\":\"wikitable\"})\n",
    "        tables = soup.find_all(\"table\", {\"class\":[\"wikitable\", \"infobox vevent\"]})\n",
    "        \n",
    "        if tables != []:\n",
    "            try:\n",
    "                # build temp dicts to dump into temp list that dumps into overall super list \n",
    "                df_dict = {}\n",
    "                temp_list = []\n",
    "\n",
    "                for table in tables:\n",
    "\n",
    "                    str_tab = str(table)\n",
    "\n",
    "                    #one-off replace to fix html error\n",
    "                    str_tab = str_tab.replace('6;', '6')\n",
    "\n",
    "                    temp_df = pd.read_html(str_tab)[0]\n",
    "                    temp_list.append(temp_df)\n",
    "\n",
    "                df_dict[key] = temp_list\n",
    "                df_dict[\"url\"] = url\n",
    "                \n",
    "                # add table df to super list\n",
    "                super_list_dict.append(df_dict) \n",
    "                \n",
    "                #success counter\n",
    "                success +=1\n",
    "\n",
    "            except:\n",
    "                print(f\"Skipping Parsing Error for: {url}\")\n",
    "        else:\n",
    "            print(f\"No wikitables found, skipping: {url}\")\n",
    "        \n",
    "        #Display\n",
    "        if count%100 == 50:\n",
    "            print(f'**** Scraped {count} of {tot} urls ****')\n",
    "        count += 1    \n",
    "    \n",
    "    print(\"!!!! Complete !!!!\")       \n",
    "    print(f'{success} of {len(url_list)} urls have wikitables')    \n",
    "\n",
    "    return super_list_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lister(soup):\n",
    "    result = []\n",
    "    uls = soup.find_all('ul', {'class': 'mw-allpages-chunk'})\n",
    "    base =\"https://en.wikipedia.org\"\n",
    "    for ul in uls:\n",
    "        for li in ul.find_all('li'):\n",
    "            for link in li.find_all('a'):\n",
    "                url = link.get('href')\n",
    "                contents = link.text\n",
    "                #print(f'{base}{url} Key: {contents}')\n",
    "                temp = base+url\n",
    "                result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(urls):\n",
    "    res = []\n",
    "    for url in urls:\n",
    "        html = urlopen(url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        temp = lister(soup)\n",
    "        res.append(temp)\n",
    "\n",
    "   # res = res[0] \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_results(super_list_dict):\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"Number of Tables\", \"key\", \"url\"])\n",
    "    i = 0\n",
    "    \n",
    "    for d_ in super_list_dict:\n",
    "        key = [x for x in d_.keys()][0]\n",
    "        url = d_.get('url', \"None\")\n",
    "        num_df = len(d_[key])\n",
    "        df.loc[i] = [str(num_df), key, url]\n",
    "        i += 1\n",
    "            \n",
    "    #print(f' num df: {num_df} | key: {key} | url: {url}')\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random delay\n",
    "def decision(probability, maxdelay=8):\n",
    "    if random.random() < probability:\n",
    "        delay = random.randint(1, maxdelay)\n",
    "    else:\n",
    "        delay = 0\n",
    "    return delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search across all text for keyword\n",
    "def keyword_search(soup_results, keyword):\n",
    "    \n",
    "    search_result = []\n",
    "    outer_cnt = 0 \n",
    "    \n",
    "    for res in soup_results:\n",
    "\n",
    "        for k,v in res.items():\n",
    "            if k != \"url\":\n",
    "                key=k\n",
    "            if k == \"url\":\n",
    "                url=res[k]\n",
    "\n",
    "        inner_cnt = 0\n",
    "        for frame in res[key]:\n",
    "\n",
    "            t=frame.to_dict()\n",
    "            values = set(chain.from_iterable(i.values() for i in t.values()))\n",
    "            values = (str(values).split())\n",
    "\n",
    "            temp_result = []\n",
    "            for elem in values:\n",
    "                if keyword.lower() in elem.lower():\n",
    "\n",
    "                    temp_result = [outer_cnt, inner_cnt, key, url]\n",
    "\n",
    "            if temp_result != []:\n",
    "                search_result.append(temp_result)\n",
    "\n",
    "            inner_cnt += 1    \n",
    "        outer_cnt += 1\n",
    "        \n",
    "    for line in search_result:\n",
    "        print(line)\n",
    "    \n",
    "    if search_result == []:\n",
    "        return print(\"No results found\")\n",
    "        \n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN PROGRESS...\n",
    "def search_title1(keyword, df):\n",
    "    \n",
    "    print(df[df['key'].str.contains(keyword, case=False)])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN PROGRESS...\n",
    "def search_title2(keyword, url_list):\n",
    "    \n",
    "    search_words = [x.split(\"/\")[-1].lower() for x in url_list]\n",
    "\n",
    "    index = []\n",
    "    for words in search_word:\n",
    "        temp_index = []\n",
    "        if keyword.lower() in words:\n",
    "            ind = search_words.index(words)\n",
    "            index.append(ind)\n",
    "            \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of Ethiopian urls to scrape for tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of column headers you want (reference the \"SELECT\" line in Query assignment)\n",
    "headers = ['cid','cidLabel','article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SPARQL Query \n",
    "sparql_query = \"\"\"\n",
    "PREFIX schema: <http://schema.org/>\n",
    "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "SELECT ?cid ?country ?article WHERE {\n",
    "\n",
    "   #?cid wdt:P31 wd:Q3839081 .\n",
    "   \n",
    "   ?cid  wdt:P17 wd:Q115 .\n",
    "   \n",
    "   OPTIONAL {\n",
    "      ?cid rdfs:label ?country filter (lang(?country) = \"en\") .\n",
    "    }\n",
    "    OPTIONAL {\n",
    "      ?article schema:about ?cid .\n",
    "      ?article schema:inLanguage \"en\" .\n",
    "      ?article schema:isPartOf <https://en.wikipedia.org/> .\n",
    "      #?article schema:isPartOf <https://en.wikipedia.org/ .\n",
    "    }\n",
    "} \n",
    "\"\"\"\n",
    "\n",
    "results = return_sparql_query_results(sparql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform wikidata search results to list of urls to scrape for tables\n",
    "df = de_dupe(niceify(results), 'article')\n",
    "df_urls = df['article']\n",
    "wikidata_urls = list(df_urls)\n",
    "wikidata_urls[:5] + [\"etc.....\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape wikipedia for Ethiopian pages: https://en.wikipedia.org/wiki/Special:AllPages?from=ethiopia&to=&namespace=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs below are from a manual search from the link above, the 3 pages have ETH-related pages\n",
    "url1 = 'https://en.wikipedia.org/w/index.php?title=Special:AllPages&from=Ethiopia'\n",
    "url2 = 'https://en.wikipedia.org/w/index.php?title=Special:AllPages&from=Ethiopian+Civil+Aviation+Agency'\n",
    "url3 = 'https://en.wikipedia.org/w/index.php?title=Special:AllPages&from=Ethiopian+farming'\n",
    "\n",
    "urls=[url1, url2,url3]\n",
    "\n",
    "wikipedia_urls = get_all_urls(urls)\n",
    "wikipedia_urls = [i for g in wikipedia_urls for i in g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the wikidata and wikipedia url lists together\n",
    "url_list = wikidata_urls + wikipedia_urls\n",
    "\n",
    "#Clean out trailing urls not associated with ETH\n",
    "ind = url_list.index('https://en.wikipedia.org/wiki/Ethiopiochamus_centralis')\n",
    "\n",
    "url_list = url_list[:ind]\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beautiful soup to scrape list of wikipedia pages' urls and return any tables that are on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup_results = table_scrapes(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta between urls scraped and urls with at least one wikitable\n",
    "print(f'{len(soup_results)} of {len(url_list)} urls have wikitables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results of urls WITH tables\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"max_rows\", None)\n",
    "df = display_all_results(soup_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enter keyword to search for TABLE DATA:\n",
    "keyword = \"farm\"\n",
    "\n",
    "search = keyword_search(soup_results, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_results[176]['Healthcare_in_Ethiopia'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u= ['https://en.wikipedia.org/wiki/2021_Ethiopian_general_election']\n",
    "tt= table_scrapes(u, prob=.05, maxdelay=3)\n",
    "\n",
    "df2 = display_all_results(tt)\n",
    "df2.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tt[0]['2021_Ethiopian_general_election'][0]\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
