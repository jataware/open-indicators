{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.sparql import (get_subclasses_of_item, return_sparql_query_results)\n",
    "import pandas as pd\n",
    "import functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful Soup Scrape for Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from itertools import chain\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display options:\n",
    "pd.set_option(\"max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of Ethiopian urls to scrape for tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of column headers you want (reference the \"SELECT\" line in Query assignment)\n",
    "headers = ['item','itemLabel','article_url','instanceOfLabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SPARQL Query \n",
    "\n",
    "sparql_query = \"\"\"\n",
    "\n",
    "PREFIX schema: <http://schema.org/>\n",
    "PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "SELECT DISTINCT ?item ?itemLabel ?article_url ?instanceOfLabel #?subLabel\n",
    "\n",
    "WHERE {\n",
    "  \n",
    "  # item = sovereign state = Ethiopia\n",
    "  ?item wdt:P17 wd:Q115 .\n",
    "  \n",
    "  # the item's property \"instance of\" \n",
    "  ?item wdt:P31 ?instanceOf . \n",
    "\n",
    "  OPTIONAL {\n",
    "    \n",
    "    # the item's \"subclass of\" category: commented out. filters out too many results\n",
    "    #?item wdt:P279 ?sub .\n",
    "    \n",
    "    ?article_url schema:about ?item .\n",
    "    ?article_url schema:inLanguage \"en\" .\n",
    "    ?article_url schema:isPartOf <https://en.wikipedia.org/> . }\n",
    "  \n",
    "  SERVICE wikibase:label { \n",
    "    bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\".\n",
    "    ?item rdfs:label ?itemLabel . \n",
    "    ?article_url rdfs:label ?article_url_label . \n",
    "    ?instanceOf rdfs:label ?instanceOfLabel . \n",
    "    ?sub rdfs:label ?subLabel .\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "results = return_sparql_query_results(sparql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEAN WIKIDATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = fn.clean_wikidata(results, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of urls to scrape from wikidata\n",
    "wikidata_urls = df_clean[[\"article_url\", \"instanceOfLabel\"]].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPE and CLEAN Wikipedia: \n",
    "https://en.wikipedia.org/wiki/Special:AllPages?from=ethiopia&to=&namespace=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the manually discovered wikipedia pages (related to Ethiopia) for tagged categories\n",
    "wikipedia_urls = fn.scrape_wikipedia(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join wikidata and wikipedia search lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean out trailing urls not associated with Ethiopia...via manual inspection:\n",
    "wikipedia_urls = wikipedia_urls[:958]\n",
    "\n",
    "# Add the wikidata and wikipedia url lists together with property tags\n",
    "url_list_full = wikidata_urls + wikipedia_urls\n",
    "print(f'Total urls to scrape for tables: {len(url_list_full)}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape wikipedia pages' urls and return any tables that are on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# table_scrapes('list of urls to scrape', 'probability of delay', 'max delay')\n",
    "# delay prevents search from being blocked\n",
    "\n",
    "# THIS SCRAPE TAKES ~1 HOUR TO COMPLETE\n",
    "\n",
    "super_list = fn.table_scrapes(url_list_full, .1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display articles with number of relevant tables:\n",
    "df_all = fn.display_all_results(super_list)\n",
    "\n",
    "# clean up list structure of properties\n",
    "df_all['properties'] = df_all.properties.apply(lambda x: x)\n",
    "df_all.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore some of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "super_list[45]['Gilgel_Gibe_I_Dam'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Some Search Options (work in progess; not yet refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GEOSPATIAL search for a keyword \n",
    "keyword = \"cattle\"\n",
    "coord = fn.keyword_search(super_list, keyword)\n",
    "point = fn.keyword_search(super_list, keyword)\n",
    "loc = fn.keyword_search(super_list, keyword)\n",
    "t = fn.keyword_search(super_list, keyword)\n",
    "\n",
    "for tt in loc:\n",
    "    print(tt[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt_list = []\n",
    "coord_list = []\n",
    "loc_list =[]\n",
    "for pt in point:\n",
    "    pt_list.append(pt[0])\n",
    "for pt in coord:\n",
    "    coord_list.append(pt[0])\n",
    "for pt in loc:\n",
    "    loc_list.append(pt[0])    \n",
    "ind_list = pt_list + coord_list+loc_list    \n",
    "geo_de = set(ind_list)\n",
    "\n",
    "print(f'point: {len(pt_list)}')\n",
    "print(f'coord: {len(coord_list)}')\n",
    "print(f'loc: {len(loc_list)}') \n",
    "print(f'set: {len(geo_de)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GEOSPATIAL TABLES\n",
    "count = 0 \n",
    "for tab in coord:\n",
    "    ind, num, key = tab[0], tab[1], tab[2]\n",
    "    temp = super_list[ind][key][num]\n",
    "    display(HTML(temp.to_html()))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORAL\n",
    "keywords = ['time', 'date', 'months', 'year', \"day\", \"founded\"]\n",
    "\n",
    "timer = fn.keyword_search(super_list, \"time\")\n",
    "dater = fn.keyword_search(super_list, \"date\")\n",
    "dayer = fn.keyword_search(super_list, \"day\")\n",
    "monther = fn.keyword_search(super_list, \"month\")\n",
    "yearer = fn.keyword_search(super_list, \"year\")\n",
    "founder = fn.keyword_search(super_list, \"founded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in monther:\n",
    "    ind = t[0]\n",
    "    num = t[1]\n",
    "    key = t[2]\n",
    "    \n",
    "    tab = super_list[ind][key][num]\n",
    "    display(HTML(tab.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'time: {len(timer)}')\n",
    "print(f'date: {len(dater)}')\n",
    "print(f'day: {len(dayer)}')\n",
    "print(f'month: {len(monther)}')\n",
    "print(f'year: {len(yearer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search \"Properties ONLY\" for Keywords\n",
    "keywords = [\"election\"]\n",
    "ind = 0\n",
    "ind_holder = []\n",
    "for prop in df_all['properties']:\n",
    "    dup = False\n",
    "    for word in keywords:\n",
    "        if (word in prop) and (dup == False):\n",
    "            ind_holder.append(ind)\n",
    "            dup = True\n",
    "    ind += 1    \n",
    "    \n",
    "for ind in ind_holder:\n",
    "    for key in super_list[ind].keys():\n",
    "        if key != \"url\" and key != \"properties\":\n",
    "            k = key \n",
    "        tab = super_list[ind][k][0]\n",
    "    display(HTML(tab.to_html()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
